{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import csv\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "imdb = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "label=[]\n",
    "tweet=[]\n",
    "tsvfile = open('Apple-Twitter-Sentiment-DFE.csv')\n",
    "reader = csv.reader(tsvfile, delimiter=',')\n",
    "for row in reader:\n",
    "        #print(type(row[5]))\n",
    "        r=[]\n",
    "        l=[]\n",
    "        if row[5]!='3' :\n",
    "                for i in (row[11].split()):\n",
    "                    try:\n",
    "                        r.append(word_index[i])\n",
    "                    except KeyError:\n",
    "                        r.append(word_index[\"<UNK>\"])\n",
    "                if row[5]=='5':\n",
    "                    label.append(1)\n",
    "                elif row[5]=='1':\n",
    "                    label.append(0)\n",
    "                if row[5]!='not_relevant':\n",
    "                    data.append(r)\n",
    "                    tweet.append(row[11])\n",
    "del data[0]\n",
    "traindata=np.array(data[:1400])\n",
    "valdata=np.array(data[1400:1642])\n",
    "del data\n",
    "trainlabel=np.array(label[:1400])\n",
    "vallabel=np.array(label[1400:1642])\n",
    "del label\n",
    "valtweet=tweet[1400:1642]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = keras.preprocessing.sequence.pad_sequences(traindata,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=40)\n",
    "\n",
    "valdata = keras.preprocessing.sequence.pad_sequences(valdata,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anoth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 16)            1600000   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 1,600,417\n",
      "Trainable params: 1,600,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 100000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size,16,input_length=40))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anoth\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "1400/1400 [==============================] - 2s 1ms/sample - loss: 0.6750 - acc: 0.7529\n",
      "Epoch 2/40\n",
      "1400/1400 [==============================] - 1s 702us/sample - loss: 0.6506 - acc: 0.7529\n",
      "Epoch 3/40\n",
      "1400/1400 [==============================] - ETA: 0s - loss: 0.6256 - acc: 0.749 - 1s 735us/sample - loss: 0.6232 - acc: 0.7529\n",
      "Epoch 4/40\n",
      "1400/1400 [==============================] - 1s 936us/sample - loss: 0.5958 - acc: 0.7529\n",
      "Epoch 5/40\n",
      "1400/1400 [==============================] - 1s 626us/sample - loss: 0.5728 - acc: 0.7529\n",
      "Epoch 6/40\n",
      "1400/1400 [==============================] - 1s 622us/sample - loss: 0.5599 - acc: 0.7529\n",
      "Epoch 7/40\n",
      "1400/1400 [==============================] - 1s 622us/sample - loss: 0.5552 - acc: 0.7529\n",
      "Epoch 8/40\n",
      "1400/1400 [==============================] - 1s 651us/sample - loss: 0.5520 - acc: 0.7529\n",
      "Epoch 9/40\n",
      "1400/1400 [==============================] - 1s 605us/sample - loss: 0.5490 - acc: 0.7529s - loss: 0.5552 - acc: 0.7\n",
      "Epoch 10/40\n",
      "1400/1400 [==============================] - 1s 580us/sample - loss: 0.5458 - acc: 0.7529\n",
      "Epoch 11/40\n",
      "1400/1400 [==============================] - 1s 602us/sample - loss: 0.5424 - acc: 0.7529\n",
      "Epoch 12/40\n",
      "1400/1400 [==============================] - 1s 667us/sample - loss: 0.5377 - acc: 0.7529\n",
      "Epoch 13/40\n",
      "1400/1400 [==============================] - 1s 709us/sample - loss: 0.5331 - acc: 0.7529\n",
      "Epoch 14/40\n",
      "1400/1400 [==============================] - 1s 586us/sample - loss: 0.5278 - acc: 0.7529s - loss: 0.5354 - acc: 0\n",
      "Epoch 15/40\n",
      "1400/1400 [==============================] - 1s 587us/sample - loss: 0.5212 - acc: 0.7529s - loss: 0.5216 - acc: 0.752\n",
      "Epoch 16/40\n",
      "1400/1400 [==============================] - 1s 611us/sample - loss: 0.5134 - acc: 0.7529\n",
      "Epoch 17/40\n",
      "1400/1400 [==============================] - 1s 605us/sample - loss: 0.5042 - acc: 0.7529s - loss: 0.4758 - ac\n",
      "Epoch 18/40\n",
      "1400/1400 [==============================] - 1s 620us/sample - loss: 0.4927 - acc: 0.7529\n",
      "Epoch 19/40\n",
      "1400/1400 [==============================] - 1s 587us/sample - loss: 0.4792 - acc: 0.7529s - loss: 0.4766 - acc: 0\n",
      "Epoch 20/40\n",
      "1400/1400 [==============================] - 1s 601us/sample - loss: 0.4619 - acc: 0.7529\n",
      "Epoch 21/40\n",
      "1400/1400 [==============================] - 1s 592us/sample - loss: 0.4420 - acc: 0.7529\n",
      "Epoch 22/40\n",
      "1400/1400 [==============================] - 1s 597us/sample - loss: 0.4218 - acc: 0.7529s - loss: 0.4934 -\n",
      "Epoch 23/40\n",
      "1400/1400 [==============================] - 1s 635us/sample - loss: 0.4030 - acc: 0.7529\n",
      "Epoch 24/40\n",
      "1400/1400 [==============================] - 1s 760us/sample - loss: 0.3866 - acc: 0.7529\n",
      "Epoch 25/40\n",
      "1400/1400 [==============================] - 1s 692us/sample - loss: 0.3718 - acc: 0.7529\n",
      "Epoch 26/40\n",
      "1400/1400 [==============================] - 1s 632us/sample - loss: 0.3582 - acc: 0.7529\n",
      "Epoch 27/40\n",
      "1400/1400 [==============================] - 1s 670us/sample - loss: 0.3467 - acc: 0.7529\n",
      "Epoch 28/40\n",
      "1400/1400 [==============================] - 1s 645us/sample - loss: 0.3347 - acc: 0.7529\n",
      "Epoch 29/40\n",
      "1400/1400 [==============================] - 1s 675us/sample - loss: 0.3244 - acc: 0.7529\n",
      "Epoch 30/40\n",
      "1400/1400 [==============================] - 1s 647us/sample - loss: 0.3137 - acc: 0.7529\n",
      "Epoch 31/40\n",
      "1400/1400 [==============================] - 1s 634us/sample - loss: 0.3052 - acc: 0.8243\n",
      "Epoch 32/40\n",
      "1400/1400 [==============================] - 1s 592us/sample - loss: 0.2943 - acc: 0.8821\n",
      "Epoch 33/40\n",
      "1400/1400 [==============================] - 1s 693us/sample - loss: 0.2816 - acc: 0.8921\n",
      "Epoch 34/40\n",
      "1400/1400 [==============================] - 1s 646us/sample - loss: 0.2661 - acc: 0.9100\n",
      "Epoch 35/40\n",
      "1400/1400 [==============================] - 1s 635us/sample - loss: 0.2519 - acc: 0.9164\n",
      "Epoch 36/40\n",
      "1400/1400 [==============================] - 1s 636us/sample - loss: 0.2386 - acc: 0.9221\n",
      "Epoch 37/40\n",
      "1400/1400 [==============================] - 1s 626us/sample - loss: 0.2259 - acc: 0.9329\n",
      "Epoch 38/40\n",
      "1400/1400 [==============================] - 1s 620us/sample - loss: 0.2127 - acc: 0.9336\n",
      "Epoch 39/40\n",
      "1400/1400 [==============================] - 1s 637us/sample - loss: 0.2011 - acc: 0.9471\n",
      "Epoch 40/40\n",
      "1400/1400 [==============================] - 1s 688us/sample - loss: 0.1899 - acc: 0.9407\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(traindata,\n",
    "                    trainlabel,\n",
    "                    epochs=40,\n",
    "                    batch_size=100,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 450us/sample - loss: 0.7530 - acc: 0.7025\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(valdata, vallabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=model.predict(valdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in s:\n",
    "    if i>=0.5:\n",
    "        t.append(1)\n",
    "    else:\n",
    "        t.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =open('PredictionDouble.csv', mode='w')\n",
    "fieldnames = ['Tweet', 'Actual', 'Predicted']\n",
    "writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "writer.writeheader()\n",
    "for i in range(0,len(s)):\n",
    "       writer.writerow({'Tweet': valtweet[i], 'Actual': vallabel[i], 'Predicted':t[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
